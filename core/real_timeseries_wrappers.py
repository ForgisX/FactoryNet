from __future__ import annotations

"""
Wrappers that expose real-world datasets in the internal `TelemetryData` format.

Each iterator here yields `TelemetryData` instances so that real datasets can
be combined with the synthetic ones generated by `TimeseriesGenerator`.
"""

from pathlib import Path
from typing import Dict, Iterable, List, Optional, Sequence

import csv

import numpy as np
import kagglehub


from telemetry_dataset import TelemetryData 

def _compute_stats(values: np.ndarray) -> Dict[str, float]:
    return {
        "mean": float(np.mean(values)),
        "std": float(np.std(values)),
        "min": float(np.min(values)),
        "max": float(np.max(values)),
    }


def _ensure_path(path: str | Path) -> Path:
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"Expected dataset path does not exist: {p}")
    return p


def _resolve_kaggle_dataset(owner: str, dataset: str) -> Path:
    """
    Try to resolve a Kaggle dataset path.

    We first ask kagglehub to download/locate it; if that fails (e.g. no
    network), we fall back to the local cache under ~/.cache/kagglehub.
    """
    handle = f"{owner}/{dataset}"
    try:
        path = kagglehub.dataset_download(handle)
        return Path(path)
    except Exception:
        cache_root = (
            Path.home() / ".cache" / "kagglehub" / "datasets" / owner / dataset / "versions"
        )
        if cache_root.exists():
            versions = sorted(p for p in cache_root.iterdir() if p.is_dir())
            if versions:
                return versions[-1]
        raise RuntimeError(
            f"Could not resolve Kaggle dataset '{handle}'. "
            "Ensure it is downloaded or pass `root` explicitly."
        )


# Every time there is a new dataset to be added, just add a new iterator function here.



# ---------------------------------------------------------------------------
# Kaggle: shivamb/machine-predictive-maintenance-classification
# ---------------------------------------------------------------------------

def iter_shivamb_predictive_maintenance(
    root: Optional[str | Path] = None,
    numeric_columns: Optional[Sequence[str]] = None,
    domain: str = "real",
    subtype: str = "predictive_maintenance_shivamb",
) -> Iterable[TelemetryData]:
    """
    Yield telemetry series from the Machine Predictive Maintenance dataset.

    We treat each selected numeric column as a separate univariate time series,
    indexed by the `UDI` column.
    """
    if root is None:
        root = _resolve_kaggle_dataset(
            "shivamb", "machine-predictive-maintenance-classification"
        )
    csv_path = _ensure_path(Path(root) / "predictive_maintenance.csv")

    if numeric_columns is None:
        numeric_columns = [
            "Air temperature [K]",
            "Process temperature [K]",
            "Rotational speed [rpm]",
            "Torque [Nm]",
            "Tool wear [min]",
        ]

    # Prepare containers
    series_values: Dict[str, List[float]] = {col: [] for col in numeric_columns}
    timestamps: List[float] = []

    with csv_path.open("r", newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            # Use UDI as a simple time index
            timestamps.append(float(row["UDI"]))
            for col in numeric_columns:
                raw = row.get(col)
                if raw is None or raw == "":
                    series_values[col].append(np.nan)
                else:
                    series_values[col].append(float(raw))

    t = np.asarray(timestamps, dtype=np.float32)

    for col, values in series_values.items():
        v = np.asarray(values, dtype=np.float32)
        stats = _compute_stats(v[~np.isnan(v)]) if np.any(~np.isnan(v)) else {
            "mean": 0.0,
            "std": 0.0,
            "min": 0.0,
            "max": 0.0,
        }
        meta = {
            "domain": domain,
            "subtype": subtype,
            "source_dataset": "machine-predictive-maintenance-classification",
            "source_column": col,
        }
        yield TelemetryData(
            id=f"shivamb_{col}",
            time=t,
            timeseries=v,
            metadata=meta,
            statistics=stats,
        )


# ---------------------------------------------------------------------------
# Kaggle: stephanmatzka/predictive-maintenance-dataset-ai4i-2020
# ---------------------------------------------------------------------------

def iter_ai4i_2020(
    root: Optional[str | Path] = None,
    numeric_columns: Optional[Sequence[str]] = None,
    domain: str = "real",
    subtype: str = "predictive_maintenance_ai4i",
) -> Iterable[TelemetryData]:
    """
    Yield telemetry series from the AI4I 2020 predictive maintenance dataset.

    We again treat each selected numeric column as a separate univariate
    time series indexed by the `UDI` column.
    """
    if root is None:
        root = _resolve_kaggle_dataset(
            "stephanmatzka", "predictive-maintenance-dataset-ai4i-2020"
        )
    csv_path = _ensure_path(Path(root) / "ai4i2020.csv")

    if numeric_columns is None:
        numeric_columns = [
            "Air temperature [K]",
            "Process temperature [K]",
            "Rotational speed [rpm]",
            "Torque [Nm]",
            "Tool wear [min]",
            "Machine failure",
        ]

    series_values: Dict[str, List[float]] = {col: [] for col in numeric_columns}
    timestamps: List[float] = []

    with csv_path.open("r", newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            timestamps.append(float(row["UDI"]))
            for col in numeric_columns:
                raw = row.get(col)
                if raw is None or raw == "":
                    series_values[col].append(np.nan)
                else:
                    series_values[col].append(float(raw))

    t = np.asarray(timestamps, dtype=np.float32)

    for col, values in series_values.items():
        v = np.asarray(values, dtype=np.float32)
        stats = _compute_stats(v[~np.isnan(v)]) if np.any(~np.isnan(v)) else {
            "mean": 0.0,
            "std": 0.0,
            "min": 0.0,
            "max": 0.0,
        }
        meta = {
            "domain": domain,
            "subtype": subtype,
            "source_dataset": "predictive-maintenance-dataset-ai4i-2020",
            "source_column": col,
        }
        yield TelemetryData(
            id=f"ai4i_{col}",
            time=t,
            timeseries=v,
            metadata=meta,
            statistics=stats,
        )


# ---------------------------------------------------------------------------
# Kaggle: vinayak123tyagi/bearing-dataset (NASA bearing)
# ---------------------------------------------------------------------------

def iter_nasa_bearing(
    root: Optional[str | Path] = None,
    domain: str = "real",
    subtype: str = "bearing",
) -> Iterable[TelemetryData]:
    """
    Yield univariate time series from the NASA bearing vibration dataset.

    Each text file contains multiple channels; we expose each channel from
    each file as a separate TelemetryData instance.
    """
    if root is None:
        root = _resolve_kaggle_dataset("vinayak123tyagi", "bearing-dataset")
    base = _ensure_path(root)

    sets = [
        ("1", base / "1st_test" / "1st_test", ["B1_x", "B1_y", "B2_x", "B2_y", "B3_x", "B3_y", "B4_x", "B4_y"]),
        ("2", base / "2nd_test" / "2nd_test", ["B1", "B2", "B3", "B4"]),
        ("3", base / "3rd_test" / "4th_test" / "txt", ["B1", "B2", "B3", "B4"]),
    ]

    for set_id, set_dir, channels in sets:
        set_dir = _ensure_path(set_dir)
        for file_path in sorted(set_dir.iterdir()):
            if not file_path.is_file():
                continue

            data = np.loadtxt(file_path, delimiter="\t")
            if data.ndim == 1:
                data = data[:, None]

            n_samples, n_cols = data.shape
            t = np.arange(n_samples, dtype=np.float32)

            for idx, ch in enumerate(channels[:n_cols]):
                values = data[:, idx].astype(np.float32)
                stats = _compute_stats(values)
                meta = {
                    "domain": domain,
                    "subtype": subtype,
                    "source_dataset": "NASA_bearing",
                    "set": set_id,
                    "channel": ch,
                    "filename": file_path.name,
                }
                yield TelemetryData(
                    id=f"NASA_set{set_id}_{ch}_{file_path.stem}",
                    time=t,
                    timeseries=values,
                    metadata=meta,
                    statistics=stats,
                )
